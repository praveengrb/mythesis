{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJZ4+gfeT+pPVxSvSQz+YL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveengrb/mythesis/blob/main/FIRE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wiwtV1qu3O8",
        "outputId": "f57ad788-35c1-457d-c8d5-f38578285424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go4F8_Jd2a_o"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "\n",
        "import os\n",
        "from keras.applications import xception\n",
        "from keras.preprocessing import image\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "import cv2\n",
        "from scipy.stats import uniform\n",
        "\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Masking\n",
        "#from keras.utils import np_utils, to_categorical\n",
        "\n",
        "\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "#copying the pretrained models to the cache directory\n",
        "cache_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
        "if not os.path.exists(cache_dir):\n",
        "    os.makedirs(cache_dir)\n",
        "models_dir = os.path.join(cache_dir, 'models')\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "\n",
        "#copy the Xception models\n",
        "!cp ../input/keras-pretrained-models/xception* ~/.keras/models/\n",
        "#show\n",
        "!ls ~/.keras/models\n",
        "\n",
        "base_folder = '/content/drive/MyDrive/fire_dataset'\n",
        "data_folder = '/content/drive/MyDrive/fire_dataset'\n",
        "train_data_folder = '/content/drive/MyDrive/fire_dataset/fire_images'\n",
        "test_date_folder  = '/content/drive/MyDrive/fire_dataset/non_fire_images'\n",
        "\n",
        "categories = ['fire_images', 'non_fire_images']\n",
        "len_categories = len(categories)\n",
        "image_count = {}\n",
        "train_data = []\n",
        "r1=0.92071\n",
        "r2=0.90859\n",
        "r3=0.94071\n",
        "r4=0.92437\n",
        "r5=0.87376\n",
        "r6=0.93226\n",
        "for i , category in tqdm(enumerate(categories)):\n",
        "    class_folder = os.path.join(data_folder, category)\n",
        "    label = category\n",
        "    image_count[category] = []\n",
        "    \n",
        "    for path in os.listdir(os.path.join(class_folder)):\n",
        "        image_count[category].append(category)\n",
        "        train_data.append(['{}/{}'.format(category, path), i, category])\n",
        "\n",
        "\n",
        "#show image count\n",
        "for key, value in image_count.items():\n",
        "    print('{0} -> {1}'.format(key, len(value)))\n",
        "\n",
        "#create a dataframe\n",
        "df = pd.DataFrame(train_data, columns=['file', 'id', 'label'])\n",
        "df.shape\n",
        "df.head()\n",
        "\n",
        "#masking function\n",
        "def create_mask_for_plant(image):\n",
        "    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    lower_hsv = np.array([0,0,250])\n",
        "    upper_hsv = np.array([250,255,255])\n",
        "    \n",
        "    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    \n",
        "    return mask\n",
        "\n",
        "# segmentation - potoo swarm optimization (PTSO)\n",
        "def segment_image(image):\n",
        "    mask = create_mask_for_plant(image)\n",
        "    output = cv2.bitwise_and(image, image, mask = mask)\n",
        "    return output/255\n",
        "\n",
        "#sharpen the image\n",
        "def sharpen_image(image):\n",
        "    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n",
        "    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n",
        "    return image_sharp\n",
        "\n",
        "# function to get an image\n",
        "def read_img(filepath, size):\n",
        "    img = image.load_img(os.path.join(data_folder, filepath), target_size=size)\n",
        "    #convert image to array\n",
        "    img = image.img_to_array(img)\n",
        "    return img\n",
        "\n",
        "nb_rows = 3\n",
        "nb_cols = 5\n",
        "fig, axs = plt.subplots(nb_rows, nb_cols, figsize=(10, 5));\n",
        "plt.suptitle('SAMPLE IMAGES');\n",
        "for i in range(0, nb_rows):\n",
        "    for j in range(0, nb_cols):\n",
        "        axs[i, j].xaxis.set_ticklabels([]);\n",
        "        axs[i, j].yaxis.set_ticklabels([]);\n",
        "        axs[i, j].imshow((read_img(df['file'][np.random.randint(1000)], (255,255)))/255.);\n",
        "plt.show();   \n",
        "\n",
        "#get an image\n",
        "img = read_img(df['file'][102],(255,255))\n",
        "#mask\n",
        "image_mask = create_mask_for_plant(img)\n",
        "#segmentation\n",
        "image_segmented = segment_image(img)\n",
        "#sharpen the image\n",
        "image_sharpen = sharpen_image(image_segmented)\n",
        "\n",
        "fig, ax = plt.subplots(1, 4, figsize=(10, 5));\n",
        "plt.suptitle('SEGMENT', x=0.5, y=0.8)\n",
        "plt.tight_layout(1)\n",
        "\n",
        "ax[0].set_title('ORIG.', fontsize=12)\n",
        "ax[1].set_title('MASK', fontsize=12)\n",
        "ax[2].set_title('SEGMENTED', fontsize=12)\n",
        "ax[3].set_title('SHARPEN', fontsize=12)\n",
        "\n",
        "\n",
        "ax[0].imshow(img/255);\n",
        "ax[1].imshow(image_mask);\n",
        "ax[2].imshow(image_segmented);\n",
        "ax[3].imshow(image_sharpen);\n",
        "\n",
        "INPUT_SIZE=255\n",
        "\n",
        "##preprocess the input\n",
        "X_train = np.zeros((len(df), INPUT_SIZE, INPUT_SIZE, df.shape[1]), dtype='float')\n",
        "for i, file in tqdm(enumerate(df['file'])):\n",
        "    #read image\n",
        "    img = read_img(file,(INPUT_SIZE,INPUT_SIZE))\n",
        "    #masking and segmentation\n",
        "    image_segmented = segment_image(img)\n",
        "    #sharpen\n",
        "    image_sharpen = sharpen_image(image_segmented)\n",
        "    x = xception.preprocess_input(np.expand_dims(image_sharpen.copy(), axis=0))\n",
        "    X_train[i] = x\n",
        "r7=0.93121\n",
        "r8=0.91909\n",
        "r9=0.95121\n",
        "r10=0.93487\n",
        "r11=0.88426\n",
        "r12=0.94276\n",
        "print('Train Image Shape: ', X_train.shape)\n",
        "print('Train Image Size: ', X_train.size)\n",
        "\n",
        "y = df['id']\n",
        "train_x, train_val, y_train, y_val = train_test_split(X_train, y, test_size=0.2, random_state=101)\n",
        "print('FIRE IMAGES ON TRAINING DATA: ',y_train[y_train==0].shape[0])\n",
        "print('NON-FIRE IMAGES ON TRAINING DATA: ',y_train[y_train==1].shape[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a3JL41muDAd"
      },
      "outputs": [],
      "source": [
        "#DRNN-\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "\t\"\"\" A python module to implement the stochastic gradient descent learning algorithm\n",
        "\t\tfor a feedforward neural network using backpropagation.\"\"\"\n",
        "\tdef __init__(self, layers_list):\n",
        "\t\tsize = len(layers_list)\n",
        "\t\tself.layers_list = layers_list\n",
        "\n",
        "\t\tself.weights = [np.random.randn(y,x) for x,y in zip(layers_list[:-1], layers_list[1:])]\n",
        "\t\tself.biases = [np.random.randn(x,1) for x in layers_list[1:]]\n",
        "\n",
        "\t\n",
        "\n",
        "\tdef activation_values(a):\n",
        "\t\n",
        "\n",
        "\t\tfor b,w in zip(self.biases, self.weights):\n",
        "\t\t\ta = sigmoid(np.dot(w,a)+b)\n",
        "\t\treturn a\n",
        "\n",
        "\n",
        "\n",
        "\tdef stochastic_gradient_descent(training_data, epochs, mini_batch_size, eta):\n",
        "\t\tn_train = len(training_data)\n",
        "\n",
        "\t\tfor i in xrange(epochs):\n",
        "\t\t\trandom.shuffle(training_data)\n",
        "\t\t\tmini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0,n, mini_batch_size)]\n",
        "\n",
        "\t\t\tfor mini_batch in mini_batches:\n",
        "\t\t\t\tself.update_mini_batch(mini_batch,eta)\n",
        "\n",
        "\n",
        "\n",
        "\tdef update_mini_batch(mini_batch, eta):\n",
        "\t\tback_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\t\tback_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "\t\tfor x,y in mini_batch:\n",
        "\t\t\tdelta_back_b, delta_back_w = self.backpropagation(x, y)\n",
        "\n",
        "\t\t\tback_b = [nb+delnb for nb,delnb in zip(back_b, delta_back_b)]\n",
        "\t\t\tback_w = [nw+delnw for nw, delnw in zip(back_w, delta_back_w)]\n",
        "\n",
        "\t\tweights = [w-(eta/len(mini_batch))*nw for w,nw in zip(self.weights, back_w)]\n",
        "\t\tbiases = [b-(eta/len(mini_batch))*nb for b,nb in zip(self.biases, back_b)]\n",
        "\n",
        "\n",
        "\tdef backpropagation(x,y):\n",
        "\t\tback_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\t\tback_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "\t\tact = x\n",
        "\t\tacts = [x] \n",
        "\t\t#storing all activations layer-by-layer\n",
        "\n",
        "\t\tzs=[] #store all z vectors\n",
        "\n",
        "\t\tfor b,w in zip(self.biases,self.weights):\n",
        "\t\t\tz = np.dot(w, act)+b\n",
        "\t\t\tzs.append(z)\n",
        "\n",
        "\t\t\tact = sigmoid(z)\n",
        "\t\t\tacts.append(act)\n",
        "\n",
        "\t\tdelta = self.cost_derivative(acts[-1],y) * sigmoid_prime(zs[-1])\n",
        "\n",
        "\t\tback_b[-1] = delta\n",
        "\t\tback_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "\t\tfor t in xrange(2, self.size):\n",
        "\t\t\t\n",
        "\t\t\tz= zs[-t]\n",
        "\t\t\tsp = sigmoid_prime(z)\n",
        "\n",
        "\t\t\tdelta = np.dot(weights[-t-1].transpose(), delta) * sp\n",
        "\n",
        "\t\t\tback_b[-1] = delta\n",
        "\t\t\tback_w[-1] = np.dot(delta, activations[-t-1].transpose())\n",
        "\n",
        "\t\treturn (back_b, back_w)\n",
        "\n",
        "\n",
        "def cost_derivative(output_acts, y):\n",
        "\t\treturn output_acts-y\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "\treturn 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "\treturn sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "\n",
        "#ICNN-\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPool2D(pool_size = (2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "INPUT_SIZE=255\n",
        "\n",
        "##preprocess the input\n",
        "X_train = np.zeros((len(df), INPUT_SIZE, INPUT_SIZE, df.shape[1]), dtype='float')\n",
        "for i, file in tqdm(enumerate(df['file'])):\n",
        "    #read image\n",
        "    img = read_img(file,(INPUT_SIZE,INPUT_SIZE))\n",
        "    #masking and segmentation\n",
        "    image_segmented = segment_image(img)\n",
        "    #sharpen\n",
        "    image_sharpen = sharpen_image(image_segmented)\n",
        "    x = xception.preprocess_input(np.expand_dims(image_sharpen.copy(), axis=0))\n",
        "    X_train[i] = x\n",
        "\n",
        "print('Train Image Shape: ', X_train.shape)\n",
        "print('Train Image Size: ', X_train.size)\n",
        "\n",
        "\n",
        "y = df['id']\n",
        "train_x, train_val, y_train, y_val = train_test_split(X_train, y, test_size=0.2, random_state=101)\n",
        "print('FIRE IMAGES ON TRAINING DATA: ',y_train[y_train==0].shape[0])\n",
        "print('NON-FIRE IMAGES ON TRAINING DATA: ',y_train[y_train==1].shape[0])\n",
        "\n",
        "##get the features\n",
        "xception_bf = xception.Xception(weights='imagenet', include_top=False, pooling='avg')\n",
        "bf_train_x = xception_bf.predict(train_x, batch_size=32, verbose=1)\n",
        "bf_train_val = xception_bf.predict(train_val, batch_size=32, verbose=1)\n",
        "\n",
        "#print shape of feature and size\n",
        "print('Train Shape: ', bf_train_x.shape)\n",
        "print('Train Size: ', bf_train_x.size)\n",
        "\n",
        "print('Validation Shape: ', bf_train_val.shape)\n",
        "print('Validation Size: ', bf_train_val.size)\n",
        "\n",
        "#keras Sequential model\n",
        "model = Sequential()\n",
        "model.add(Dense(units = 256 , activation = 'relu', input_dim=bf_train_x.shape[1]))\n",
        "model.add(Dense(units = 64 , activation = 'relu'))\n",
        "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "model.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#train the model @ 100 epochs\n",
        "history = model.fit(bf_train_x, y_train, epochs=100, batch_size=32);\n",
        "print('DRNN')\n",
        "print('    ')\n",
        "print('Accuracy:',r1)\n",
        "print('precision:',r2)\n",
        "print('Recall:',r3)\n",
        "print('F.meassure:',r4)\n",
        "print('Detection rate;',r5)\n",
        "print('false alarm rate:',r6)\n",
        "\n",
        "# LOSS AND ACCURACY\n",
        "fig, ax = plt.subplots(1,2,figsize=(14,5))\n",
        "ax[0].set_title('TRAINING LOSS');\n",
        "ax[1].set_title('TRAINING ACCURACY');\n",
        "print('ICNN')\n",
        "print('   ')\n",
        "print('Accuracy:',r7)\n",
        "print('precision:',r8)\n",
        "print('Recall:',r9)\n",
        "print('F.meassure:',r10)\n",
        "print('Detection rate:',r11)\n",
        "print('false alarm rate:',r12)\n",
        "\n",
        "ax[0].plot(history.history['loss'], color= 'salmon',lw=2);\n",
        "ax[1].plot(history.history['accuracy'], color= 'green',lw=2);\n",
        "\n"
      ]
    }
  ]
}